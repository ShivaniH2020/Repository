{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled14.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNmQV7bJjTaqJV5JB3jjXfU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShivaniH2020/Repository/blob/main/Assignment5Part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4ffPgvRKZL8"
      },
      "source": [
        "print(\"Roll no: 20MCI0001\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wK480NwMpx2"
      },
      "source": [
        "print(\"https://github.com/ShivaniH2020/Repository\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-pV7vW4Mp0Z"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "iBGNEA68Mp2T",
        "outputId": "3e3685e0-576e-4fb5-dc8a-2da8c943a0b7"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QtkNuesJMqJN",
        "outputId": "c7c9363e-0fde-4ce0-fef3-e0b78e79f4e5"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vHqhC6PaMqLp",
        "outputId": "28ea7255-cb3b-4517-da74-ba844fafbf9a"
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vCgjxXRSMqPI",
        "outputId": "bd67d3be-83f9-464a-b385-6dd07d7bfc8f"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4V05yY8KMqRc",
        "outputId": "8d4ae0d5-4447-4006-fed1-916bbdea1810"
      },
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zbgzuXYMqTw"
      },
      "source": [
        "ids_from_chars = preprocessing.StringLookup(\n",
        "    vocabulary=list(vocab))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gy6nnX3vMqVi",
        "outputId": "3897cd9a-e549-4b58-e130-550493c53122"
      },
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[41, 42, 43, 44, 45, 46, 47], [64, 65, 66]]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyNaWXdCMqXl"
      },
      "source": [
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IicV4EATNGzR",
        "outputId": "c2d9235b-3845-4800-d7e8-4a6b5a21c1e4"
      },
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "K-JY6VvKNG2D",
        "outputId": "513596ba-b281-4f3e-84c4-179d38b99600"
      },
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YY9T5xR4NG4h"
      },
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MwQdIeOeNG6T",
        "outputId": "e520e29c-80d2-44b6-a55d-e7a768a76724"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([20, 49, 58, ..., 47, 10,  2])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyLT55H5NG8d"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "kZv6q_4FNG-6",
        "outputId": "96712a30-d367-42c4-8747-0b401b11a993"
      },
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hikl8OqiNTlG"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DPuQbNUCNTnw",
        "outputId": "18742431-367a-4937-9b41-dc67c85634d1"
      },
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jXj7xYDaNTqV",
        "outputId": "3a81d4b8-6e9e-487f-e190-f2734f76e53c"
      },
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XUUhK9sNTsZ"
      },
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "GeNt3BYaNTul",
        "outputId": "0cf22b65-c473-4f26-d44d-2969155c7d54"
      },
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO0ychHENTxL"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UrBCacsmNkXl",
        "outputId": "d135d7d4-fc69-476a-ce0f-59ac5fe5b925"
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qLtUi57NNkaH",
        "outputId": "9eaa005c-0a4a-498b-bbb4-fc77727b2f71"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaIOu7O-Nscd"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3XVNxWNNse4"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mpgUt2hNshf"
      },
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "l5yOIpwlNsj4",
        "outputId": "d8d79d09-2a14-4588-a96a-f4b7f0ac3436"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 67) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fndPEijiNsmE",
        "outputId": "8547602a-b340-4a75-8416-36efb270c571"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  17152     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  68675     \n",
            "=================================================================\n",
            "Total params: 4,024,131\n",
            "Trainable params: 4,024,131\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuelBekzNsoK"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Ir6HJP5ONsqU",
        "outputId": "4603014a-6e85-4d69-eba1-c7d73d47fcec"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([30,  8, 59, 14, 53, 48, 29, 40, 21, 19, 30, 16, 54, 24, 41, 38, 57,\n",
              "       32, 53, 36, 21,  9, 65, 51, 62, 55, 14,  7, 49, 18, 52, 63, 38, 36,\n",
              "       47, 35, 11, 42, 49, 27, 30, 41, 23,  0, 50, 11, 27, 34, 45, 10, 29,\n",
              "       42,  0, 65, 38,  7, 15, 48, 46, 40, 24,  0,  9, 38,  8, 37, 12, 15,\n",
              "       20, 19, 22, 31, 58, 51, 47, 34,  0,  0, 25, 33, 42,  9, 35, 52,  6,\n",
              "       45, 22, 32,  0, 33, 38, 18, 28, 17, 16, 17, 64, 31, 47, 17])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QywHv8YhNssq",
        "outputId": "852340ad-2302-49da-b92a-abc62f275b48"
      },
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b'dustry and courage might have saved?\\nAh, what a shame! ah, what a fault were this!\\nSay Warwick was o'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"P,s?mhOZGEPBnJaXqRmVG-ykvo?'iDlwXVgU3biMPaIj3MTe.ObyX'AhfZJ-X,W:AFEHQrkgTKSb-Ul&eHRSXDNCBCxQgC\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5TGe-vJNsu_"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RGHDC0ozNsxA",
        "outputId": "aa96bade-8fc4-4d6a-ce0d-6b00d14dd56c"
      },
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 67)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.203936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "plbdcAoWOBMh",
        "outputId": "d594028f-487f-4be4-ef37-9ae2236d8086"
      },
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.94933"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LLP6bcAOBPF"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZf5QUAnOBRZ"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEtTac-GOBTq"
      },
      "source": [
        "EPOCHS = 20"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3vleXaSjOBV0",
        "outputId": "aba8a660-e3f0-4c5d-8986-0c37b5277ebb"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 918s 5s/step - loss: 3.3034\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 916s 5s/step - loss: 2.0859\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 916s 5s/step - loss: 1.7591\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 919s 5s/step - loss: 1.5755\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 916s 5s/step - loss: 1.4653\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 929s 5s/step - loss: 1.3916\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 928s 5s/step - loss: 1.3340\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 946s 5s/step - loss: 1.2838\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 906s 5s/step - loss: 1.2440\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 882s 5s/step - loss: 1.2038\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 883s 5s/step - loss: 1.1614\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 880s 5s/step - loss: 1.1187\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 880s 5s/step - loss: 1.0761\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 876s 5s/step - loss: 1.0306\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 878s 5s/step - loss: 0.9803\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 884s 5s/step - loss: 0.9306\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 886s 5s/step - loss: 0.8788\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 887s 5s/step - loss: 0.8239\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 881s 5s/step - loss: 0.7748\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 884s 5s/step - loss: 0.7218\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6mNXqY3OBX7"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['', '[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhaE7xAsOP1p"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CtPkmm_7OP4G",
        "outputId": "8429ad87-6f7d-44ae-fa46-9811879204cb"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "How well dost thou, prince?\n",
            "\n",
            "First Citizen:\n",
            "First, ay me form; for in Barnardine's sought\n",
            "Was in the prison, a half, ay a' well first,\n",
            "That thou bear me: let me have this good venge:\n",
            "'Thanks, being thus: and if I die, no souls of a\n",
            "promity of his pheact, that name's worth.\n",
            "\n",
            "TRANIO:\n",
            "Is this\n",
            "Unherit have done all the sister's death,\n",
            "Which well appeareth by this air, and Montague's beft;\n",
            "More fatherlies made down: the eagles will soin\n",
            "Of offences we look'd for with our love as mine.\n",
            "\n",
            "FLORIZEL:\n",
            "Where did you honoursay,\n",
            "Knocknive to Elbow, that when I saw inkeed\n",
            "We assure her virtuous are me--\n",
            "Wherein their obsequisarle forefores!\n",
            "I humbly revelles this newly harmony\n",
            "Which sents commandures he pricking him to do wet her sake, who\n",
            "Tarry of His master's faces,--how their state shall not?\n",
            "If any strife have throw with thus?\n",
            "Therefore this ill-soul, thy father's life,\n",
            "Made issue judgment a ruining stude,\n",
            "Whose loves I pawn'd and said 'Ay.'\n",
            "\n",
            "COMINIUS:\n",
            "Forswear it, signior Baptista, lest your ha \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.6398556232452393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JBU3ING0OP6A",
        "outputId": "4d63d7bb-cf3f-4b9e-ae0c-302567407bd4"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nWhat with a quarrel say is left her so like an indiver\\nTo comfortable thing? moves her\\nAwaked, being comfort of earth,\\nSlain and my arrival and my blood at hand?\\nHad said of parly, there it so.'\\n\\nKING RICHARD II:\\nUnsaint whom thou stay'st not thy most profound sit\\nThat thou hear'st straight enso: and I wish\\nyou buy a sister, black, how help af all,\\nDew comforts not; and Rominates my lady,\\nMake yours with those that have asleep.\\n\\nProvost:\\nMore than thou reing Lewis, think you? it shall not warrant\\nThe sepators be put forth your form.\\nMethio, design my language to me:\\nBeing but a rotten hazard's shop,\\nAs thou hast fairly in London,\\nWe are like their mother, whom the king is deal.\\n\\nOXFORD:\\nHave you been to's for up, that dance bethink\\nThe needless volume to teap,\\nWhich love I have.\\n\\nISABELLA:\\nAy, an the faction's commonwealte,\\nIn anny, in those that thou lust,--O,\\ntherein, sir, was little less in heaven\\nWhich as a feaster soul's wit, I.\\n\\nGONZALO:\\nMy father said, hear me, as our poderal S\"\n",
            " b\"ROMEO:\\nMore light sake, being the inforced this,\\nportener to yourself, and what he would not,\\nWould thou reigning heard me speaks,\\nAnd nothing come and weary soil's!\\nWith love I give three civil and all with honour.\\n\\nHursd:\\nMarry, I wonder! I saw him me again.\\n\\nSICINIUS:\\nIt is poor Kate, sir! a saw too;\\nOne accuse my old father ried, a vengeance of my fault,\\nPurtunate this bloody whiles,\\nI have resire my faces, of saints;\\nBut 'tis no time to bear a wench!\\n\\nPage:\\nGive me your crown:\\nWe's very hearting weeds and person.\\n\\nMost Sure If\\nRegardly\\nAnd not best many a cat-dack of bones\\nLing like to be thus;' a bold\\nfall'd park:\\nThe night leave his, and makes my flesh-or back?\\nThat are their spurs, stallests in old enemy.\\nMasters he had said one buildwed to his mouth,\\nWhich revenge's bosom from the neighton? O my study,\\nWhere is your answer?\\n\\nDUKE VINCENTIO:\\nNot at all tongue's. I must confed the corse.\\n\\nQUEEN ELIZABETH:\\nAnd men of Chrison.\\nWherefore do you awful out this?\\nMock wine! Now are our hors\"\n",
            " b\"ROMEO:\\nGo of that charmer, that doth ever from your blood,\\nWhich straits for them as great, in happy souls,\\nLoud is a settled as an enemy's house.\\nMy uncle Rivers, shall absent that time\\nThis divided as victors and herself,\\nTo be brief that noise of day,\\nThat thou hast forsworn thy tops and succeeds of the\\nself, that the which yourself stands, all poor Harry Margery,\\nWho, as the most are greater so? farewell.\\n\\nVIRGILIA:\\nRespect, might be the wrong,\\nAnd straight of suspicion for a king!\\nA vice fond would not speak another so:\\nLet blood fast creets upon it, for your heart,\\nBut prisoner than you before ever a\\ndangerous thing. But I warrant it so.\\n\\nPARIS:\\nSweet disbance and methon, are I see them here be absolved\\nAt some to ever came flowered.\\n\\nMERCUTIO:\\nO third me, good sirre! by the veiners, bliss and honour.\\nNow bate your majesty to equal:\\nLook wherein Clifford and Lancaster\\nRatelf, the brief and the shady-father\\nMade of my father's face, the dozard hearts\\nMay be put mine. Love, as I blow to-\"\n",
            " b\"ROMEO:\\nThe vaultness of the stire-divine, notice,\\nWho every storms are many Warwicks hither.\\n\\nPOLIXENES:\\nHis copitals. You have done all.\\n\\nMERCUTIO:\\nThe grief have power from Marshal's cauch of spirit,\\nWhich seems a glory of the north,\\nWhere one life-sever, is made an unagain?\\n\\nBAPTISTA:\\nGentlemen, fellow, to your good sword is shared about,\\nAnd says her quickly smalls arrighted line\\nAnd he his country's please.\\n\\nHENRY BOLINGBROKE:\\nYour honour and your justice shall quickly thieves her,\\nMade it my thrive to be chosen time\\nWhen you shall be--this simple and the swaft\\nTo Baptista's kindred and Angelo\\nDo can tell have followers him that:\\nO'er lady courbs, and like an emblances for\\nThe ped-cle-ping day,\\nBetween this island; and so am I thought,\\nMay it possess'd By rove her pawrance.\\n\\nISABELLA:\\nO, choose must retire of it,\\nUnto your highness' somposition, and take profits\\nWhich time 'Whoop'd down false after,\\nTo catch a sister, old patrimies,\\nAnd all that spirit, best valour in thee.\\n\\nHORTENSIO:\\n\"\n",
            " b\"ROMEO:\\nGo take this house, the thing I have, thou liest.\\n\\nLucy Citizen:\\nWell, sir, your mistress is well created.\\n\\nFRANCISCA:\\nAh, what?\\nWhat then? what's the matter?\\n\\nCURTIS:\\nIs he had quarrel's holy said which I do for you;\\nHorse is my Lord Northumberland?\\n\\nPETRUCHIO:\\nI saw thee, if you'll know.\\n\\nDUKE VINCENTIO:\\nWell, genall: and he's not doubtful service.\\n\\nBUSHY:\\nMarry, sir, I have reason was a fresh armoss\\nMy flaxture hath misare; and have we then straight:\\nToo much poysed with the sea, and kiss you,\\nSo and hast thy wars, he will do good.\\n\\nPETRUCHIO:\\nTurn me in this country, when thou dost often hold\\nShould be some unlike full day's means,\\nFamiliar, and like alike down instruction.\\nMake me assare you? Why that's no blaof?\\n\\nWARWICK:\\nThen yields\\nThe palm of saints, receive it should\\nBe safe you'll keep it, good durst I do for;\\nMy falaurs she hath often be a greater;\\nFor I will choose you forswear no.\\n\\nPROSPERO:\\nHow now! let's have we rather.\\n\\nLUCENTIO:\\nAnd he softly kiss his study.\\n\\nESCALUS\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.313058853149414\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eezGIbhPOP8X",
        "outputId": "00cdc773-de79-4993-c36d-611b466ff3e3"
      },
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f085ea60fd0>, because it is not built.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ITC4VvPwOP-o",
        "outputId": "9b9ccbfe-2a58-4171-daf5-adba490126d2"
      },
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7f085c6950e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7f085c6950e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x7f085c6950e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x7f085c6950e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "The Dort that I am haw.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "Thou know'st; I'll take my heart with holy prayer\n",
            "Than wh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d25tGj3QOQAk"
      },
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KA0wAKZPK1d"
      },
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAaIq25IPK4I"
      },
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Mk2I7MQwPK6Y",
        "outputId": "c6619ed0-a4fd-4d1b-ae57-81b1f7196bd8"
      },
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "172/172 [==============================] - 891s 5s/step - loss: 2.7145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f085e1215d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "STIx6P9dPK9G",
        "outputId": "ec52271d-e487-4045-bacd-08d4e27630b6"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1807\n",
            "Epoch 1 Batch 50 Loss 2.0876\n",
            "Epoch 1 Batch 100 Loss 1.9480\n",
            "Epoch 1 Batch 150 Loss 1.8518\n",
            "\n",
            "Epoch 1 Loss: 1.9881\n",
            "Time taken for 1 epoch 886.39 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.7862\n",
            "Epoch 2 Batch 50 Loss 1.7105\n",
            "Epoch 2 Batch 100 Loss 1.6885\n",
            "Epoch 2 Batch 150 Loss 1.6867\n",
            "\n",
            "Epoch 2 Loss: 1.7088\n",
            "Time taken for 1 epoch 889.63 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.5714\n",
            "Epoch 3 Batch 50 Loss 1.5716\n",
            "Epoch 3 Batch 100 Loss 1.5337\n",
            "Epoch 3 Batch 150 Loss 1.4691\n",
            "\n",
            "Epoch 3 Loss: 1.5462\n",
            "Time taken for 1 epoch 880.27 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4333\n",
            "Epoch 4 Batch 50 Loss 1.4699\n",
            "Epoch 4 Batch 100 Loss 1.4369\n",
            "Epoch 4 Batch 150 Loss 1.4208\n",
            "\n",
            "Epoch 4 Loss: 1.4479\n",
            "Time taken for 1 epoch 881.34 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.4193\n",
            "Epoch 5 Batch 50 Loss 1.3402\n",
            "Epoch 5 Batch 100 Loss 1.4214\n",
            "Epoch 5 Batch 150 Loss 1.3510\n",
            "\n",
            "Epoch 5 Loss: 1.3803\n",
            "Time taken for 1 epoch 893.11 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3540\n",
            "Epoch 6 Batch 50 Loss 1.3261\n",
            "Epoch 6 Batch 100 Loss 1.3344\n",
            "Epoch 6 Batch 150 Loss 1.3544\n",
            "\n",
            "Epoch 6 Loss: 1.3287\n",
            "Time taken for 1 epoch 889.01 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2773\n",
            "Epoch 7 Batch 50 Loss 1.2836\n",
            "Epoch 7 Batch 100 Loss 1.2590\n",
            "Epoch 7 Batch 150 Loss 1.3007\n",
            "\n",
            "Epoch 7 Loss: 1.2837\n",
            "Time taken for 1 epoch 884.30 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2586\n",
            "Epoch 8 Batch 50 Loss 1.2185\n",
            "Epoch 8 Batch 100 Loss 1.2546\n",
            "Epoch 8 Batch 150 Loss 1.2727\n",
            "\n",
            "Epoch 8 Loss: 1.2429\n",
            "Time taken for 1 epoch 885.40 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1541\n",
            "Epoch 9 Batch 50 Loss 1.2031\n",
            "Epoch 9 Batch 100 Loss 1.2250\n",
            "Epoch 9 Batch 150 Loss 1.1893\n",
            "\n",
            "Epoch 9 Loss: 1.2027\n",
            "Time taken for 1 epoch 886.07 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1451\n",
            "Epoch 10 Batch 50 Loss 1.1561\n",
            "Epoch 10 Batch 100 Loss 1.1657\n",
            "Epoch 10 Batch 150 Loss 1.1798\n",
            "\n",
            "Epoch 10 Loss: 1.1638\n",
            "Time taken for 1 epoch 884.65 sec\n",
            "________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sCE7BuhOQDA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}